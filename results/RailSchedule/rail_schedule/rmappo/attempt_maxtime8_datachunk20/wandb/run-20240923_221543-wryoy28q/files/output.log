observation_space [Box(9,), Box(9,), Box(9,)]
share_observation_space [Box(27,), Box(27,), Box(27,)]
observation_space [Box(9,), Box(9,), Box(9,)]
share_observation_space [Box(27,), Box(27,), Box(27,)]
obs_space:  [Box(9,), Box(9,), Box(9,)]
share_obs_space:  [Box(27,), Box(27,), Box(27,)]
act_space:  Discrete(3)

 Scenario rail_schedule Algo rmappo Exp attempt_maxtime8_datachunk20 updates 0/520 episodes, total num timesteps 1920/1000000.0, FPS 1356.

average episode rewards is -318815.595703125
> /home/chunhuili/Transportation/RailEnvironment/MAPPO/runner/rail_runner.py(148)eval()
-> eval_episode_rewards = []
> /home/chunhuili/Transportation/RailEnvironment/MAPPO/runner/rail_runner.py(149)eval()
-> eval_obs = self.eval_envs.reset()
> /home/chunhuili/Transportation/RailEnvironment/MAPPO/runner/rail_runner.py(151)eval()
-> eval_rnn_states = np.zeros((self.n_eval_rollout_threads, *self.buffer.rnn_states.shape[2:]), dtype=np.float32)
> /home/chunhuili/Transportation/RailEnvironment/MAPPO/runner/rail_runner.py(152)eval()
-> eval_masks = np.ones((self.n_eval_rollout_threads, self.num_agents, 1), dtype=np.float32)
> /home/chunhuili/Transportation/RailEnvironment/MAPPO/runner/rail_runner.py(154)eval()
-> for eval_step in range(self.episode_length):
> /home/chunhuili/Transportation/RailEnvironment/MAPPO/runner/rail_runner.py(155)eval()
-> self.trainer.prep_rollout()
> /home/chunhuili/Transportation/RailEnvironment/MAPPO/runner/rail_runner.py(156)eval()
-> eval_action, eval_rnn_states = self.trainer.policy.act(np.concatenate(eval_obs),
> /home/chunhuili/Transportation/RailEnvironment/MAPPO/runner/rail_runner.py(157)eval()
-> np.concatenate(eval_rnn_states),
> /home/chunhuili/Transportation/RailEnvironment/MAPPO/runner/rail_runner.py(158)eval()
-> np.concatenate(eval_masks),
> /home/chunhuili/Transportation/RailEnvironment/MAPPO/runner/rail_runner.py(159)eval()
-> deterministic=True)
> /home/chunhuili/Transportation/RailEnvironment/MAPPO/runner/rail_runner.py(156)eval()
-> eval_action, eval_rnn_states = self.trainer.policy.act(np.concatenate(eval_obs),
RuntimeError: shape '[32, 3]' is invalid for input of size 3
> /home/chunhuili/Transportation/RailEnvironment/MAPPO/runner/rail_runner.py(156)eval()
-> eval_action, eval_rnn_states = self.trainer.policy.act(np.concatenate(eval_obs),
*** NameError: name 'eval_action' is not defined
(32, 3, 9)
Traceback (most recent call last):
  File "main.py", line 128, in <module>
    runner.writter.close()
  File "main.py", line 113, in main
    }
  File "/home/chunhuili/Transportation/RailEnvironment/MAPPO/runner/rail_runner.py", line 79, in run
    self.eval(total_num_steps)
  File "/home/chunhuili/anaconda3/envs/transport/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/chunhuili/Transportation/RailEnvironment/MAPPO/runner/rail_runner.py", line 156, in eval
    eval_action, eval_rnn_states = self.trainer.policy.act(np.concatenate(eval_obs),
  File "/home/chunhuili/anaconda3/envs/transport/lib/python3.8/bdb.py", line 94, in trace_dispatch
    return self.dispatch_exception(frame, arg)
  File "/home/chunhuili/anaconda3/envs/transport/lib/python3.8/bdb.py", line 174, in dispatch_exception
    if self.quitting: raise BdbQuit
bdb.BdbQuit
